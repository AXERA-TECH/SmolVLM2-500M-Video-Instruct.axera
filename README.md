# SmolVLM2-500M-Video-Instruct.axera

> HuggingFaceTB SmolVLM2-500M-Video-Instruct DEMO on Axera.

- ç›®å‰æ”¯æŒ `Python` è¯­è¨€, `C++` ä»£ç åœ¨å¼€å‘ä¸­.
- é¢„ç¼–è¯‘æ¨¡å‹å¯ä»¥ä»[ç™¾åº¦ç½‘ç›˜]()ä¸‹è½½.
- å¦‚éœ€è‡ªè¡Œå¯¼å‡ºç¼–è¯‘ `VIT` æ¨¡å‹è¯·å‚è€ƒ [æ¨¡å‹è½¬æ¢](/model_convert/README.md).

## æ”¯æŒå¹³å°

- [x] AX650N
- [ ] AX630C

## Git Clone

é¦–å…ˆä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ `clone` æœ¬é¡¹ç›®, ç„¶åè¿›å…¥ `python` æ–‡ä»¶å¤¹:

```bash
$ git clone git@github.com:AXERA-TECH/SmolVLM2-500M-Video-Instruct.axera.git
$ cd SmolVLM2-500M-Video-Instruct.axera/python
```

ä¹‹ååœ¨å¼€å‘æ¿ä¸Šä¸‹è½½æˆ–å®‰è£…ä»¥ä¸‹æ”¯æŒåº“:

- ä» `huggingface` ä¸‹è½½ `SmolVLM2-500M-Video-Instruct` æ¨¡å‹.

    ```bash
    $ git clone https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct
    ```

- åœ¨å¼€å‘æ¿ä¸Šå®‰è£…é…ç½® `pyaxengine`, [ç‚¹å‡»è·³è½¬ä¸‹è½½é“¾æ¥](https://github.com/AXERA-TECH/pyaxengine/releases). æ³¨æ„æ¿ç«¯ `SDK` æœ€ä½ç‰ˆæœ¬è¦æ±‚:

    - AX650 SDK >= 2.18
    - AX620E SDK >= 3.12
    - æ‰§è¡Œ `pip3 install axengine-x.x.x-py3-none-any.whl` å®‰è£…

å°†ä¸‹è½½åçš„é¢„ç¼–è¯‘æ¨¡å‹è§£å‹åˆ°å½“å‰æ–‡ä»¶å¤¹[ğŸ””å¯é€‰], é»˜è®¤æ–‡ä»¶å¤¹æ’å¸ƒå¦‚ä¸‹:

```bash
(lerobot) âœ  python git:(master) âœ— tree -L 2 .
.
â”œâ”€â”€ infer_axmodel.py
â”œâ”€â”€ infer.py
â”œâ”€â”€ SmolVLM2-500M-Video-Instruct
â”‚Â Â  â”œâ”€â”€ added_tokens.json
â”‚Â Â  â”œâ”€â”€ chat_template.json
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ generation_config.json
â”‚Â Â  â”œâ”€â”€ merges.txt
â”‚Â Â  â”œâ”€â”€ model.safetensors
â”‚Â Â  â”œâ”€â”€ onnx
â”‚Â Â  â”œâ”€â”€ preprocessor_config.json
â”‚Â Â  â”œâ”€â”€ processor_config.json
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
â”‚Â Â  â”œâ”€â”€ tokenizer.json
â”‚Â Â  â””â”€â”€ vocab.json
â”œâ”€â”€ SmolVLM2-500M-Video-Instruct_axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l0_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l10_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l11_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l12_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l13_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l14_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l15_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l16_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l17_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l18_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l19_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l1_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l20_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l21_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l22_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l23_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l24_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l25_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l26_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l27_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l28_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l29_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l2_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l30_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l31_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l3_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l4_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l5_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l6_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l7_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l8_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l9_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_post.axmodel
â”‚Â Â  â””â”€â”€ model.embed_tokens.weight.npy
â”œâ”€â”€ SmolVLMVisionEmbeddings.pkl
â”œâ”€â”€ utils
â”‚Â Â  â””â”€â”€ infer_func.py
â””â”€â”€ vit-models
    â”œâ”€â”€ vision_model.axmodel
    â””â”€â”€ vision_model.onnx
```

## ä¸Šæ¿éƒ¨ç½²

- `AX650N` çš„è®¾å¤‡å·²é¢„è£… `Ubuntu 22.04`
- ä»¥ `root` æƒé™ç™»é™† `AX650N` çš„æ¿å¡è®¾å¤‡
- æ¥å…¥äº’è”ç½‘, ç¡®ä¿ `AX650N` çš„è®¾å¤‡èƒ½æ­£å¸¸æ‰§è¡Œ `apt install`, `pip install` ç­‰æŒ‡ä»¤
- å·²éªŒè¯è®¾å¤‡: `AX650N DEMO Board`ã€`çˆ±èŠ¯æ´¾Pro(AX650N)`

### Python API è¿è¡Œ

#### Requirements

```bash
$ mkdir /opt/site-packages
$ cd python
$ pip3 install -r requirements.txt --prefix=/opt/site-packages
``` 

#### æ·»åŠ ç¯å¢ƒå˜é‡

å°†ä»¥ä¸‹ä¸¤è¡Œæ·»åŠ åˆ° `/root/.bashrc`(å®é™…æ·»åŠ çš„è·¯å¾„éœ€è¦è‡ªè¡Œæ£€æŸ¥)å, é‡æ–°è¿æ¥ç»ˆç«¯æˆ–è€…æ‰§è¡Œ `source ~/.bashrc`

```bash
$ export PYTHONPATH=$PYTHONPATH:/opt/site-packages/local/lib/python3.10/dist-packages  
$ export PATH=$PATH:/opt/site-packages/local/bin
``` 

#### è¿è¡Œ

åœ¨ `Axera å¼€å‘æ¿` ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å¯å›¾åƒç†è§£åŠŸèƒ½:

```sh
$ cd SmolVLM2-500M-Video-Instruct.axera/python
$ python3 infer_axmodel_und.py
```

é»˜è®¤è¾“å…¥å›¾åƒä¸º:

![image.png](python/imgs/image.png)

ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ‰‹åŠ¨æŒ‡å®šå›¾åƒè·¯å¾„. æ¨¡å‹æ¨ç†ç»“æœå¦‚ä¸‹:

```bash
[INFO] Chip type: ChipType.MC50
[INFO] VNPU type: VNPUType.DISABLED
[INFO] Engine version: 2.11.0a
vit_output.shape is (1, 576, 2048), vit feature extract done!
Init InferenceSession: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:06<00:00,  3.89it/s]
model load done!
prefill done!
Decoder:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 634/1024 [00:00<00:00, 2493.31it/s]Decoder:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 733/1024 [00:18<00:09, 29.61it/s]hit eos!
Decoder:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 762/1024 [00:23<00:08, 32.02it/s]
è¿™å¹…å›¾å±•ç¤ºäº†ä¸‰ä½ç©¿ç€å®‡èˆªæœçš„å®‡èˆªå‘˜ï¼Œä»–ä»¬ç«™åœ¨ä¸€ç‰‡èŒ‚å¯†çš„æ¤è¢«ä¸­ã€‚å®‡èˆªå‘˜ä»¬çš„å¤´ç›”ä¸Šæœ‰åå…‰é¢ç½©ï¼Œå¯ä»¥çœ‹åˆ°ä»–ä»¬çš„é¢å®¹ã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡æ£®æ—ï¼Œæ ‘æœ¨å’Œæ¤ç‰©çš„ç»†èŠ‚éå¸¸æ¸…æ™°ã€‚å®‡èˆªå‘˜ä»¬çš„å§¿åŠ¿å„ä¸ç›¸åŒï¼Œå…¶ä¸­ä¸€ä½å®‡èˆªå‘˜æ­£ä¸¾èµ·åŒæ‰‹ï¼Œä¼¼ä¹åœ¨å‘æŸäººæŒ¥æ‰‹ï¼Œå¦ä¸€ä½å®‡èˆªå‘˜åˆ™ç«™ç«‹ç€ï¼Œç›®å…‰å‘å‰æ–¹çœ‹å»ï¼Œç¬¬ä¸‰ä½å®‡èˆªå‘˜åˆ™å¼¯è…°é è¿‘åœ°é¢ï¼Œä¼¼ä¹åœ¨è§‚å¯Ÿåœ°é¢ä¸Šçš„æŸç‰©ã€‚æ•´ä½“ç”»é¢ç»™äººä¸€ç§ç§‘å¹»å’Œæ¢ç´¢çš„æ„Ÿè§‰ï¼Œä»¿ä½›ä»–ä»¬æ­£åœ¨è¿›è¡Œä¸€æ¬¡å¤ªç©ºæ¢é™©ä»»åŠ¡ã€‚
```

åœ¨ `Axera å¼€å‘æ¿` ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤å®ç°å›¾åƒç”Ÿæˆ:

```sh
$ cd SmolVLM2-500M-Video-Instruct.axera/python
$ python3 infer_axmodel_gen.py
```

é¢„è®¾ `prompt` ä¸º: `"A close-up high-contrast photo of Sydney Opera House sitting next to Eiffel tower, under a blue night sky of roiling energy, exploding yellow stars, and radiating swirls of blue."`

ç”Ÿæˆçš„å›¾åƒä¿å­˜åœ¨ `./generated_samples/` æ–‡ä»¶å¤¹ä¸‹:

![output](assets/gen_out_img.jpg)

#### å›¾åƒç†è§£ä»»åŠ¡Â·æ¨ç†è€—æ—¶ç»Ÿè®¡

Model | Time |
---| ---|
ImageEncoder | 142.682 ms |
Prefill TTFT | 4560.214 ms |
Decoder | 87.48 ms |

å…¶ä¸­:

- `Prefill` é˜¶æ®µ, æ¯ä¸€å±‚çš„ `llama_layer` å¹³å‡è€—æ—¶ `189.565 ms`.
- `Decoder` é˜¶æ®µ, æ¯ä¸€å±‚çš„ `llama_layer` å¹³å‡è€—æ—¶ `3.201` ms.
- `llama_post` è€—æ—¶ `10.654 ms`.

æ¨¡å‹è§£ç é€Ÿåº¦ä¸º: 1000 / 87.48ms = 11.43 tokens/s.

#### å›¾åƒç”Ÿæˆä»»åŠ¡Â·æ¨ç†è€—æ—¶ç»Ÿè®¡ (1 token)

Model | Time |
---| ---|
llama prefill g1 | 189.565 ms * 24 * 2 |
llama decode g0 | 3.201 ms * 24 * 2 |
norm & gen_head | 40 ms
gen_aligner | 2.0 ms

ç”Ÿæˆ `384x384` åˆ†è¾¨ç‡çš„å›¾åƒé»˜è®¤ä½¿ç”¨ `576` ä¸ª `token` (1 ä¸ª prefill + 575 decode).

æœ€åä½¿ç”¨ `gen_vision_model_decode` è·å–å›¾åƒç»“æœ, è¯¥æ¨¡å—è€—æ—¶ `17507.68 ms`.

## æŠ€æœ¯è®¨è®º

- Github issues
- QQ ç¾¤: 139953715
