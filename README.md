# SmolVLM2-500M-Video-Instruct.axera

> HuggingFaceTB SmolVLM2-500M-Video-Instruct DEMO on Axera.

- ç›®å‰æ”¯æŒ `Python` è¯­è¨€, `C++` ä»£ç åœ¨å¼€å‘ä¸­.
- é¢„ç¼–è¯‘æ¨¡å‹å¯ä»¥ä»[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1udw7_IMQehr_2CmipfLOXw?pwd=n6qe)ä¸‹è½½.
- å¦‚éœ€è‡ªè¡Œå¯¼å‡ºç¼–è¯‘ `VIT` æ¨¡å‹è¯·å‚è€ƒ [æ¨¡å‹è½¬æ¢](/model_convert/README.md).

## æ”¯æŒå¹³å°

- [x] AX650N
- [ ] AX630C

## Git Clone

é¦–å…ˆä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ `clone` æœ¬é¡¹ç›®, ç„¶åè¿›å…¥ `python` æ–‡ä»¶å¤¹:

```bash
$ git clone git@github.com:AXERA-TECH/SmolVLM2-500M-Video-Instruct.axera.git
$ cd SmolVLM2-500M-Video-Instruct.axera/python
```

ä¹‹ååœ¨å¼€å‘æ¿ä¸Šä¸‹è½½æˆ–å®‰è£…ä»¥ä¸‹æ”¯æŒåº“:

- ä» `huggingface` ä¸‹è½½ `SmolVLM2-500M-Video-Instruct` æ¨¡å‹.

    ```bash
    $ git clone https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct
    ```

- åœ¨å¼€å‘æ¿ä¸Šå®‰è£…é…ç½® `pyaxengine`, [ç‚¹å‡»è·³è½¬ä¸‹è½½é“¾æ¥](https://github.com/AXERA-TECH/pyaxengine/releases). æ³¨æ„æ¿ç«¯ `SDK` æœ€ä½ç‰ˆæœ¬è¦æ±‚:

    - AX650 SDK >= 2.18
    - AX620E SDK >= 3.12
    - æ‰§è¡Œ `pip3 install axengine-x.x.x-py3-none-any.whl` å®‰è£…

å°†ä¸‹è½½åçš„é¢„ç¼–è¯‘æ¨¡å‹è§£å‹åˆ°å½“å‰æ–‡ä»¶å¤¹[ğŸ””å¯é€‰], é»˜è®¤æ–‡ä»¶å¤¹æ’å¸ƒå¦‚ä¸‹:

```bash
(lerobot) âœ  python git:(master) âœ— tree -L 2 .
.
â”œâ”€â”€ infer_axmodel.py
â”œâ”€â”€ infer.py
â”œâ”€â”€ SmolVLM2-500M-Video-Instruct
â”‚Â Â  â”œâ”€â”€ added_tokens.json
â”‚Â Â  â”œâ”€â”€ chat_template.json
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â”œâ”€â”€ generation_config.json
â”‚Â Â  â”œâ”€â”€ merges.txt
â”‚Â Â  â”œâ”€â”€ model.safetensors
â”‚Â Â  â”œâ”€â”€ onnx
â”‚Â Â  â”œâ”€â”€ preprocessor_config.json
â”‚Â Â  â”œâ”€â”€ processor_config.json
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ special_tokens_map.json
â”‚Â Â  â”œâ”€â”€ tokenizer_config.json
â”‚Â Â  â”œâ”€â”€ tokenizer.json
â”‚Â Â  â””â”€â”€ vocab.json
â”œâ”€â”€ SmolVLM2-500M-Video-Instruct_axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l0_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l10_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l11_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l12_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l13_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l14_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l15_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l16_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l17_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l18_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l19_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l1_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l20_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l21_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l22_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l23_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l24_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l25_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l26_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l27_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l28_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l29_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l2_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l30_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l31_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l3_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l4_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l5_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l6_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l7_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l8_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_p128_l9_together.axmodel
â”‚Â Â  â”œâ”€â”€ llama_post.axmodel
â”‚Â Â  â””â”€â”€ model.embed_tokens.weight.npy
â”œâ”€â”€ SmolVLMVisionEmbeddings.pkl
â”œâ”€â”€ utils
â”‚Â Â  â””â”€â”€ infer_func.py
â””â”€â”€ vit-models
    â”œâ”€â”€ vision_model.axmodel
    â””â”€â”€ vision_model.onnx [å¯é€‰]
```

## ä¸Šæ¿éƒ¨ç½²

- `AX650N` çš„è®¾å¤‡å·²é¢„è£… `Ubuntu 22.04`
- ä»¥ `root` æƒé™ç™»é™† `AX650N` çš„æ¿å¡è®¾å¤‡
- æ¥å…¥äº’è”ç½‘, ç¡®ä¿ `AX650N` çš„è®¾å¤‡èƒ½æ­£å¸¸æ‰§è¡Œ `apt install`, `pip install` ç­‰æŒ‡ä»¤
- å·²éªŒè¯è®¾å¤‡: `AX650N DEMO Board`ã€`çˆ±èŠ¯æ´¾Pro(AX650N)`

### Python API è¿è¡Œ

#### Requirements

```bash
$ mkdir /opt/site-packages
$ cd python
$ pip3 install -r requirements.txt --prefix=/opt/site-packages
``` 

#### æ·»åŠ ç¯å¢ƒå˜é‡

å°†ä»¥ä¸‹ä¸¤è¡Œæ·»åŠ åˆ° `/root/.bashrc`(å®é™…æ·»åŠ çš„è·¯å¾„éœ€è¦è‡ªè¡Œæ£€æŸ¥)å, é‡æ–°è¿æ¥ç»ˆç«¯æˆ–è€…æ‰§è¡Œ `source ~/.bashrc`

```bash
$ export PYTHONPATH=$PYTHONPATH:/opt/site-packages/local/lib/python3.10/dist-packages  
$ export PATH=$PATH:/opt/site-packages/local/bin
``` 

#### è¿è¡Œ

åœ¨ `Axera å¼€å‘æ¿` ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å¯å›¾åƒç†è§£åŠŸèƒ½:

```sh
$ cd SmolVLM2-500M-Video-Instruct.axera/python
$ python3 infer_axmodel.py
```

è¾“å…¥å›¾åƒä¸º:

![image.png](assets/girl.png)

é€šè¿‡å‘½ä»¤è¡Œå‚æ•°å¯ä»¥æ‰‹åŠ¨æŒ‡å®šå›¾åƒè·¯å¾„, æ¨¡å‹æ¨ç†ç»“æœå¦‚ä¸‹:

```bash
$ python3 infer_axmodel.py -i ../assets/girl.png --vit_model vit-models/vision_model.axmodel

Model loaded successfully!
slice_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8]
Slice prefill done: 0
Slice prefill done: 1
Slice prefill done: 2
Slice prefill done: 3
Slice prefill done: 4
Slice prefill done: 5
Slice prefill done: 6
Slice prefill done: 7
Slice prefill done: 8
answer >>  The image depicts a young woman with long, light gray hair, adorned with two pink flowers in her hair. She is standing on a beach, facing the camera with a neutral expression. The woman is wearing a blue off-shoulder dress that is open at the front, revealing^@ a white lace top underneath. She is also wearing a silver choker necklace and a silver bracelet on her left wrist.

The background of the image reveals a clear blue sky with fluffy white clouds, suggesting a sunny day. The ocean is visible in the distance, with gentle waves crashing onto the shore. The overall scene suggests a serene and peaceful beach setting.

The woman's attire and accessories, along with the serene ocean and clear sky, create a calm and picturesque atmosphere. The image does not contain any^@ discernible text or additional objects. The relative positions of the objects suggest that the woman is standing in the foreground, with the ocean and sky in the background. The image does not provide any information that would allow for a specific question to be answered definitively.
```

#### å›¾åƒç†è§£ä»»åŠ¡Â·æ¨ç†è€—æ—¶ç»Ÿè®¡

è¯¥æ¨¡å‹ä¸€å…±æœ‰ `32` å±‚ `decode layer`, è¯¦ç»†è€—æ—¶ä¿¡æ¯å¦‚ä¸‹:

Model | Time |
---| ---|
ImageEncoder | 1830 ms |
Prefill TTFT | 2892.151 ms |
Decoder | 27.51 ms |

å…¶ä¸­:

- `Prefill` é˜¶æ®µ, æ¯ä¸€å±‚çš„ `llama_layer` æœ€å¤§è€—æ—¶ `90.3 ms`.

    å„ä¸ªå­å›¾è€—æ—¶:

    ```sh
    g1: 3.143 ms
    g2: 4.909 ms
    g3: 6.610 ms
    g4: 8.263 ms
    g5: 9.997 ms
    g6: 11.819 ms
    g7: 13.579 ms
    g8: 15.096 ms
    g9: 16.814 ms
    ```

- `Decoder` é˜¶æ®µ, æ¯ä¸€å±‚çš„ `llama_layer` å¹³å‡è€—æ—¶ `0.780 ms` ms.
- `llama_post` è€—æ—¶ `2.551 ms`.

æ¨¡å‹è§£ç é€Ÿåº¦ä¸º: 1000 / 27.51 ms = 36.35 tokens/s.

## æŠ€æœ¯è®¨è®º

- Github issues
- QQ ç¾¤: 139953715
